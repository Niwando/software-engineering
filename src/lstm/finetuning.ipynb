{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ILuC5l44hXHr"},"outputs":[],"source":["# necessary imports\n","from google.colab import drive\n","import os\n","import glob\n","import json\n","import time\n","from datetime import timedelta\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import joblib\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import Dataset, DataLoader\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24533,"status":"ok","timestamp":1739653191914,"user":{"displayName":"Tobias Schnarr","userId":"16156372285777759423"},"user_tz":-60},"id":"9Nm2Nvoc3e3v","outputId":"12d5ec65-bfce-4a9a-b0b5-9b7db9817fce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# establish connection to google drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!git clone https://tobyaz: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@github.com/Niwando/software-engineering.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXxK9ZvnZudP","executionInfo":{"status":"ok","timestamp":1739653868837,"user_tz":-60,"elapsed":19171,"user":{"displayName":"Tobias Schnarr","userId":"16156372285777759423"}},"outputId":"0631cbf8-87d3-4212-87c8-7626c06b507a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'software-engineering'...\n","remote: Enumerating objects: 540, done.\u001b[K\n","remote: Counting objects: 100% (63/63), done.\u001b[K\n","remote: Compressing objects: 100% (39/39), done.\u001b[K\n","remote: Total 540 (delta 28), reused 44 (delta 22), pack-reused 477 (from 4)\u001b[K\n","Receiving objects: 100% (540/540), 114.68 MiB | 15.63 MiB/s, done.\n","Resolving deltas: 100% (287/287), done.\n","Updating files: 100% (241/241), done.\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","import os\n","import glob\n","import json\n","import time\n","from datetime import timedelta\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import joblib\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ------------------------------\n","# Functions for Data Processing\n","# ------------------------------\n","def load_stock_data(file_pattern: str, stock_ticker: str) -> pd.DataFrame:\n","    \"\"\"Load JSON files matching the pattern for the given stock and return a combined DataFrame.\"\"\"\n","    all_dfs = []\n","    files = glob.glob(file_pattern)\n","    for file in tqdm(files, desc=\"Loading JSON files for \" + stock_ticker):\n","        if not os.path.basename(file).startswith(stock_ticker):\n","            continue\n","        try:\n","            with open(file, 'r') as f:\n","                data = json.load(f)\n","                metadata = data[\"Meta Data\"]\n","                symbol = metadata[\"2. Symbol\"]\n","                interval = metadata[\"4. Interval\"]\n","                if interval != \"1min\":\n","                    print(f\"Warning: {file} has interval {interval} (expected '1min'). Skipping.\")\n","                    continue\n","                ts_key = f\"Time Series ({interval})\"\n","                ts_data = data.get(ts_key)\n","                if not ts_data:\n","                    print(f\"Warning: {ts_key} not found in {file}. Skipping.\")\n","                    continue\n","                df = pd.DataFrame(ts_data).T\n","                df = df.apply(pd.to_numeric)\n","                df[\"symbol\"] = symbol\n","                all_dfs.append(df)\n","        except Exception as e:\n","            print(f\"Error loading {file}: {str(e)}\")\n","            continue\n","    if not all_dfs:\n","        raise ValueError(f\"No valid data found in any files for {stock_ticker}\")\n","    combined_df = pd.concat(all_dfs)\n","    combined_df.index = pd.to_datetime(combined_df.index)\n","    combined_df = combined_df.sort_index()\n","    # Remove any prefix from column names (e.g., \"1. open\" -> \"open\")\n","    combined_df.columns = [col.split(\". \")[-1] for col in combined_df.columns]\n","    return combined_df\n","\n","def process_stock_data(df: pd.DataFrame, fill_method: str = 'ffill',\n","                       filter_market_hours: bool = True, timezone: str = 'US/Eastern') -> pd.DataFrame:\n","    \"\"\"Reindex the data to a full minute range, fill missing values, and filter for regular trading hours.\"\"\"\n","    if df.index.tz is None:\n","        df.index = df.index.tz_localize(timezone)\n","    processed_dfs = []\n","    for symbol, group in df.groupby('symbol'):\n","        group = group[~group.index.duplicated(keep='last')]\n","        full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n","        group = group.reindex(full_range)\n","        group['symbol'] = symbol\n","        if fill_method == 'ffill':\n","            group = group.ffill()\n","        elif fill_method == 'bfill':\n","            group = group.bfill()\n","        elif fill_method == 'interpolate':\n","            group = group.interpolate()\n","        else:\n","            raise ValueError(f\"Invalid fill_method: {fill_method}.\")\n","        if filter_market_hours:\n","            group = group.between_time('09:30', '16:00')\n","        processed_dfs.append(group)\n","    processed_df = pd.concat(processed_dfs)\n","    return processed_df\n","\n","def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Add intraday cyclical and daily time features to the DataFrame.\"\"\"\n","    df = df.copy()\n","    df.index = pd.to_datetime(df.index)\n","    if df.index.tz is None:\n","        df.index = df.index.tz_localize('US/Eastern')\n","    df['hour'] = df.index.hour\n","    df['minute'] = df.index.minute\n","    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n","    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n","    df['sin_minute'] = np.sin(2 * np.pi * df['minute'] / 60)\n","    df['cos_minute'] = np.cos(2 * np.pi * df['minute'] / 60)\n","    df['day_of_week'] = df.index.dayofweek\n","    df['day_of_month'] = df.index.day\n","    df['month'] = df.index.month\n","    df['day_of_year'] = df.index.dayofyear\n","    df['year'] = df.index.year\n","    return df\n","\n","def create_sequences_multi(data: np.ndarray, window_size: int, forecast_horizon: int):\n","    \"\"\"Create sequences from the data; input: past 'window_size' minutes; target: next 'forecast_horizon' 'close' values.\n","       Assumes that the 'close' column is at index 3.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(data) - window_size - forecast_horizon + 1):\n","        X.append(data[i:i+window_size])\n","        y.append(data[i+window_size:i+window_size+forecast_horizon, 3])\n","    return np.array(X), np.array(y)\n","\n","class StockDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","    def __len__(self):\n","        return len(self.y)\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# ------------------------------\n","# Model Architecture\n","# ------------------------------\n","class MultiStepLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2):\n","        super(MultiStepLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.forecast_horizon = forecast_horizon\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, forecast_horizon)\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# ------------------------------\n","# Global Parameters\n","# ------------------------------\n","stocks = [\"AAPL\", \"MSFT\", \"NVDA\", \"TSLA\", \"AMZN\", \"GOOGL\", \"META\", \"NFLX\", \"AVGO\", \"PYPL\"]\n","\n","WINDOW_SIZE = 60         # Use past 60 minutes as input\n","forecast_horizon = 60      # Predict next 60 minutes (target)\n","EPOCHS = 10\n","learning_rate = 1e-4\n","features_list = ['open', 'high', 'low', 'close', 'volume',\n","                 'sin_hour', 'cos_hour', 'sin_minute', 'cos_minute',\n","                 'day_of_week', 'day_of_month', 'month', 'day_of_year', 'year']\n","input_size = len(features_list)\n","hidden_size = 128\n","num_layers = 1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ------------------------------\n","# Process Each Stock\n","# ------------------------------\n","for stock_ticker in stocks:\n","    print(f\"\\n=== Processing stock: {stock_ticker} ===\")\n","\n","    # Define file paths for the current stock\n","    json_pattern = f\"software-engineering/src/app/api/alphavantage/data/{stock_ticker}_*.json\"\n","    historical_csv = f\"software-engineering/src/lstm/processed_data/processed_data_{stock_ticker}.csv\"\n","    scaler_file = f\"software-engineering/src/lstm/scalers/scaler_minute_{stock_ticker}.pkl\"\n","    model_file = f\"software-engineering/src/lstm/models/trained_model_{stock_ticker}.pth\"\n","    finetuned_model_file = f\"drive/MyDrive/pecunia/trained_models/finetuned/trained_model_{stock_ticker}_finetuned.pth\"\n","\n","    # 1. Load and process new JSON data\n","    try:\n","        new_raw_df = load_stock_data(json_pattern, stock_ticker)\n","    except ValueError as ve:\n","        print(f\"Skipping {stock_ticker}: {ve}\")\n","        continue\n","    new_processed_df = process_stock_data(new_raw_df, fill_method='ffill', filter_market_hours=True)\n","    new_processed_df = add_time_features(new_processed_df)\n","\n","    # 2. Update historical data (append new data and deduplicate by timestamp)\n","    if os.path.exists(historical_csv):\n","        historical_df = pd.read_csv(historical_csv, index_col='timestamp', parse_dates=True)\n","        combined_df = pd.concat([historical_df, new_processed_df])\n","        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n","        combined_df = combined_df.sort_index()\n","    else:\n","        combined_df = new_processed_df\n","    combined_df.to_csv(historical_csv, index_label='timestamp')\n","    print(f\"Historical data updated for {stock_ticker}.\")\n","\n","    # 3. Fine-tuning preparation: select last 60 days of data\n","    end_time = combined_df.index.max()\n","    start_time = end_time - pd.Timedelta(days=60)\n","    finetune_df = combined_df.loc[start_time:end_time]\n","    print(f\"Fine-tuning data time range for {stock_ticker}: {finetune_df.index.min()} to {finetune_df.index.max()}\")\n","\n","    # 4. Normalize fine-tuning data using the existing scaler\n","    if not os.path.exists(scaler_file):\n","        print(f\"Scaler file not found for {stock_ticker}. Skipping fine-tuning.\")\n","        continue\n","    scaler = joblib.load(scaler_file)\n","    finetune_norm_values = scaler.transform(finetune_df[features_list])\n","    finetune_norm_df = pd.DataFrame(finetune_norm_values, columns=features_list, index=finetune_df.index)\n","    finetune_norm_df['symbol'] = finetune_df['symbol']\n","\n","    # 5. Create training sequences for fine-tuning\n","    data_array = finetune_norm_df[features_list].values\n","    X_multi, y_multi = create_sequences_multi(data_array, WINDOW_SIZE, forecast_horizon)\n","    print(f\"Fine-tuning sequences shapes for {stock_ticker}: X: {X_multi.shape}, y: {y_multi.shape}\")\n","\n","    finetune_dataset = StockDataset(X_multi, y_multi)\n","    train_loader = DataLoader(finetune_dataset, batch_size=64, shuffle=True)\n","\n","    # 6. Load pre-trained model and fine-tune\n","    if not os.path.exists(model_file):\n","        print(f\"Pre-trained model file not found for {stock_ticker}. Skipping fine-tuning.\")\n","        continue\n","    model = MultiStepLSTMModel(input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2)\n","    model.to(device)\n","    model.load_state_dict(torch.load(model_file, map_location=device))\n","    print(f\"Pre-trained model loaded for {stock_ticker}.\")\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        total_loss = 0.0\n","        for batch_x, batch_y in train_loader:\n","            batch_x = batch_x.to(device)\n","            batch_y = batch_y.to(device)\n","            optimizer.zero_grad()\n","            output = model(batch_x)\n","            loss = criterion(output, batch_y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item() * batch_x.size(0)\n","        avg_loss = total_loss / len(finetune_dataset)\n","        print(f\"{stock_ticker} - Fine-tuning Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.6f}\")\n","\n","    # 7. Save the fine-tuned model\n","    torch.save(model.state_dict(), finetuned_model_file)\n","    print(f\"Fine-tuned model saved for {stock_ticker} to {finetuned_model_file}\")\n","\n","print(\"Fine-tuning for all stocks complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fGTqybiaSM-","executionInfo":{"status":"ok","timestamp":1739654694528,"user_tz":-60,"elapsed":292580,"user":{"displayName":"Tobias Schnarr","userId":"16156372285777759423"}},"outputId":"9de1566f-53f2-455f-9466-eafdd26bcea7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Processing stock: AAPL ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for AAPL: 100%|██████████| 13/13 [00:05<00:00,  2.25it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for AAPL. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for AAPL.\n","Fine-tuning data time range for AAPL: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for AAPL: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for AAPL.\n","AAPL - Fine-tuning Epoch 1/10, Loss: 0.061588\n","AAPL - Fine-tuning Epoch 2/10, Loss: 0.005825\n","AAPL - Fine-tuning Epoch 3/10, Loss: 0.003774\n","AAPL - Fine-tuning Epoch 4/10, Loss: 0.002749\n","AAPL - Fine-tuning Epoch 5/10, Loss: 0.002211\n","AAPL - Fine-tuning Epoch 6/10, Loss: 0.001944\n","AAPL - Fine-tuning Epoch 7/10, Loss: 0.001612\n","AAPL - Fine-tuning Epoch 8/10, Loss: 0.001463\n","AAPL - Fine-tuning Epoch 9/10, Loss: 0.001298\n","AAPL - Fine-tuning Epoch 10/10, Loss: 0.001131\n","Fine-tuned model saved for AAPL to drive/MyDrive/pecunia/trained_models/trained_model_AAPL_finetuned.pth\n","\n","=== Processing stock: MSFT ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for MSFT: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for MSFT. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for MSFT.\n","Fine-tuning data time range for MSFT: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for MSFT: X: (23342, 60, 14), y: (23342, 60)\n","Pre-trained model loaded for MSFT.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["MSFT - Fine-tuning Epoch 1/10, Loss: 0.010642\n","MSFT - Fine-tuning Epoch 2/10, Loss: 0.004606\n","MSFT - Fine-tuning Epoch 3/10, Loss: 0.003289\n","MSFT - Fine-tuning Epoch 4/10, Loss: 0.002477\n","MSFT - Fine-tuning Epoch 5/10, Loss: 0.001975\n","MSFT - Fine-tuning Epoch 6/10, Loss: 0.001669\n","MSFT - Fine-tuning Epoch 7/10, Loss: 0.001444\n","MSFT - Fine-tuning Epoch 8/10, Loss: 0.001299\n","MSFT - Fine-tuning Epoch 9/10, Loss: 0.001188\n","MSFT - Fine-tuning Epoch 10/10, Loss: 0.001102\n","Fine-tuned model saved for MSFT to drive/MyDrive/pecunia/trained_models/trained_model_MSFT_finetuned.pth\n","\n","=== Processing stock: NVDA ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for NVDA: 100%|██████████| 13/13 [00:05<00:00,  2.26it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for NVDA. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for NVDA.\n","Fine-tuning data time range for NVDA: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for NVDA: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for NVDA.\n","NVDA - Fine-tuning Epoch 1/10, Loss: 0.005671\n","NVDA - Fine-tuning Epoch 2/10, Loss: 0.001823\n","NVDA - Fine-tuning Epoch 3/10, Loss: 0.001335\n","NVDA - Fine-tuning Epoch 4/10, Loss: 0.001086\n","NVDA - Fine-tuning Epoch 5/10, Loss: 0.000950\n","NVDA - Fine-tuning Epoch 6/10, Loss: 0.000838\n","NVDA - Fine-tuning Epoch 7/10, Loss: 0.000766\n","NVDA - Fine-tuning Epoch 8/10, Loss: 0.000845\n","NVDA - Fine-tuning Epoch 9/10, Loss: 0.000812\n","NVDA - Fine-tuning Epoch 10/10, Loss: 0.000761\n","Fine-tuned model saved for NVDA to drive/MyDrive/pecunia/trained_models/trained_model_NVDA_finetuned.pth\n","\n","=== Processing stock: TSLA ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for TSLA: 100%|██████████| 13/13 [00:05<00:00,  2.28it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for TSLA. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for TSLA.\n","Fine-tuning data time range for TSLA: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for TSLA: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for TSLA.\n","TSLA - Fine-tuning Epoch 1/10, Loss: 0.426742\n","TSLA - Fine-tuning Epoch 2/10, Loss: 0.062804\n","TSLA - Fine-tuning Epoch 3/10, Loss: 0.037633\n","TSLA - Fine-tuning Epoch 4/10, Loss: 0.027373\n","TSLA - Fine-tuning Epoch 5/10, Loss: 0.021080\n","TSLA - Fine-tuning Epoch 6/10, Loss: 0.016692\n","TSLA - Fine-tuning Epoch 7/10, Loss: 0.013342\n","TSLA - Fine-tuning Epoch 8/10, Loss: 0.011022\n","TSLA - Fine-tuning Epoch 9/10, Loss: 0.009367\n","TSLA - Fine-tuning Epoch 10/10, Loss: 0.008209\n","Fine-tuned model saved for TSLA to drive/MyDrive/pecunia/trained_models/trained_model_TSLA_finetuned.pth\n","\n","=== Processing stock: AMZN ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for AMZN: 100%|██████████| 13/13 [00:06<00:00,  1.99it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for AMZN. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for AMZN.\n","Fine-tuning data time range for AMZN: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for AMZN: X: (23342, 60, 14), y: (23342, 60)\n","Pre-trained model loaded for AMZN.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["AMZN - Fine-tuning Epoch 1/10, Loss: 0.149016\n","AMZN - Fine-tuning Epoch 2/10, Loss: 0.024695\n","AMZN - Fine-tuning Epoch 3/10, Loss: 0.011849\n","AMZN - Fine-tuning Epoch 4/10, Loss: 0.008627\n","AMZN - Fine-tuning Epoch 5/10, Loss: 0.007009\n","AMZN - Fine-tuning Epoch 6/10, Loss: 0.005833\n","AMZN - Fine-tuning Epoch 7/10, Loss: 0.005010\n","AMZN - Fine-tuning Epoch 8/10, Loss: 0.004394\n","AMZN - Fine-tuning Epoch 9/10, Loss: 0.004005\n","AMZN - Fine-tuning Epoch 10/10, Loss: 0.003968\n","Fine-tuned model saved for AMZN to drive/MyDrive/pecunia/trained_models/trained_model_AMZN_finetuned.pth\n","\n","=== Processing stock: GOOGL ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for GOOGL: 100%|██████████| 13/13 [00:06<00:00,  1.92it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for GOOGL. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for GOOGL.\n","Fine-tuning data time range for GOOGL: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for GOOGL: X: (23342, 60, 14), y: (23342, 60)\n","Pre-trained model loaded for GOOGL.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["GOOGL - Fine-tuning Epoch 1/10, Loss: 0.017999\n","GOOGL - Fine-tuning Epoch 2/10, Loss: 0.002486\n","GOOGL - Fine-tuning Epoch 3/10, Loss: 0.001838\n","GOOGL - Fine-tuning Epoch 4/10, Loss: 0.001601\n","GOOGL - Fine-tuning Epoch 5/10, Loss: 0.001454\n","GOOGL - Fine-tuning Epoch 6/10, Loss: 0.001349\n","GOOGL - Fine-tuning Epoch 7/10, Loss: 0.001264\n","GOOGL - Fine-tuning Epoch 8/10, Loss: 0.001187\n","GOOGL - Fine-tuning Epoch 9/10, Loss: 0.001092\n","GOOGL - Fine-tuning Epoch 10/10, Loss: 0.000996\n","Fine-tuned model saved for GOOGL to drive/MyDrive/pecunia/trained_models/trained_model_GOOGL_finetuned.pth\n","\n","=== Processing stock: META ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for META: 100%|██████████| 13/13 [00:06<00:00,  1.88it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for META. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for META.\n","Fine-tuning data time range for META: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for META: X: (23342, 60, 14), y: (23342, 60)\n","Pre-trained model loaded for META.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["META - Fine-tuning Epoch 1/10, Loss: 0.121551\n","META - Fine-tuning Epoch 2/10, Loss: 0.025523\n","META - Fine-tuning Epoch 3/10, Loss: 0.011318\n","META - Fine-tuning Epoch 4/10, Loss: 0.006474\n","META - Fine-tuning Epoch 5/10, Loss: 0.004818\n","META - Fine-tuning Epoch 6/10, Loss: 0.003922\n","META - Fine-tuning Epoch 7/10, Loss: 0.003219\n","META - Fine-tuning Epoch 8/10, Loss: 0.002788\n","META - Fine-tuning Epoch 9/10, Loss: 0.002496\n","META - Fine-tuning Epoch 10/10, Loss: 0.002305\n","Fine-tuned model saved for META to drive/MyDrive/pecunia/trained_models/trained_model_META_finetuned.pth\n","\n","=== Processing stock: NFLX ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for NFLX: 100%|██████████| 13/13 [00:05<00:00,  2.50it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for NFLX. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for NFLX.\n","Fine-tuning data time range for NFLX: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for NFLX: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for NFLX.\n","NFLX - Fine-tuning Epoch 1/10, Loss: 0.057212\n","NFLX - Fine-tuning Epoch 2/10, Loss: 0.006272\n","NFLX - Fine-tuning Epoch 3/10, Loss: 0.004184\n","NFLX - Fine-tuning Epoch 4/10, Loss: 0.003353\n","NFLX - Fine-tuning Epoch 5/10, Loss: 0.002889\n","NFLX - Fine-tuning Epoch 6/10, Loss: 0.002530\n","NFLX - Fine-tuning Epoch 7/10, Loss: 0.002284\n","NFLX - Fine-tuning Epoch 8/10, Loss: 0.002117\n","NFLX - Fine-tuning Epoch 9/10, Loss: 0.001995\n","NFLX - Fine-tuning Epoch 10/10, Loss: 0.001891\n","Fine-tuned model saved for NFLX to drive/MyDrive/pecunia/trained_models/trained_model_NFLX_finetuned.pth\n","\n","=== Processing stock: AVGO ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for AVGO: 100%|██████████| 13/13 [00:05<00:00,  2.47it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for AVGO. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for AVGO.\n","Fine-tuning data time range for AVGO: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for AVGO: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for AVGO.\n","AVGO - Fine-tuning Epoch 1/10, Loss: 0.023293\n","AVGO - Fine-tuning Epoch 2/10, Loss: 0.007902\n","AVGO - Fine-tuning Epoch 3/10, Loss: 0.005156\n","AVGO - Fine-tuning Epoch 4/10, Loss: 0.003742\n","AVGO - Fine-tuning Epoch 5/10, Loss: 0.003073\n","AVGO - Fine-tuning Epoch 6/10, Loss: 0.002678\n","AVGO - Fine-tuning Epoch 7/10, Loss: 0.002471\n","AVGO - Fine-tuning Epoch 8/10, Loss: 0.002282\n","AVGO - Fine-tuning Epoch 9/10, Loss: 0.002121\n","AVGO - Fine-tuning Epoch 10/10, Loss: 0.002021\n","Fine-tuned model saved for AVGO to drive/MyDrive/pecunia/trained_models/trained_model_AVGO_finetuned.pth\n","\n","=== Processing stock: PYPL ===\n"]},{"output_type":"stream","name":"stderr","text":["Loading JSON files for PYPL: 100%|██████████| 13/13 [00:04<00:00,  2.68it/s]\n","<ipython-input-7-e7435e49cda1>:50: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n","  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"]},{"output_type":"stream","name":"stdout","text":["New data processed for PYPL. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n","Historical data updated for PYPL.\n","Fine-tuning data time range for PYPL: 2024-12-02 16:00:00-05:00 to 2025-01-31 16:00:00-05:00\n","Fine-tuning sequences shapes for PYPL: X: (23342, 60, 14), y: (23342, 60)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n","<ipython-input-7-e7435e49cda1>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_file, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Pre-trained model loaded for PYPL.\n","PYPL - Fine-tuning Epoch 1/10, Loss: 0.281297\n","PYPL - Fine-tuning Epoch 2/10, Loss: 0.043021\n","PYPL - Fine-tuning Epoch 3/10, Loss: 0.019442\n","PYPL - Fine-tuning Epoch 4/10, Loss: 0.013675\n","PYPL - Fine-tuning Epoch 5/10, Loss: 0.010416\n","PYPL - Fine-tuning Epoch 6/10, Loss: 0.008274\n","PYPL - Fine-tuning Epoch 7/10, Loss: 0.006880\n","PYPL - Fine-tuning Epoch 8/10, Loss: 0.005971\n","PYPL - Fine-tuning Epoch 9/10, Loss: 0.005006\n","PYPL - Fine-tuning Epoch 10/10, Loss: 0.004231\n","Fine-tuned model saved for PYPL to drive/MyDrive/pecunia/trained_models/trained_model_PYPL_finetuned.pth\n","Fine-tuning for all stocks complete.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN7iOOkGkWHS7W6CS0bHA1f"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}