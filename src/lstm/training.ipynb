{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1739663505194,
     "user": {
      "displayName": "Tobias Schnarr",
      "userId": "16156372285777759423"
     },
     "user_tz": -60
    },
    "id": "w9eN6eP-_FnT"
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from google.colab import drive\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21985,
     "status": "ok",
     "timestamp": 1739663468255,
     "user": {
      "displayName": "Tobias Schnarr",
      "userId": "16156372285777759423"
     },
     "user_tz": -60
    },
    "id": "s7bnbsiC_H8t",
    "outputId": "ad5cc784-0f3b-4e08-d648-108f643dc4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# establish connection to google drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 11041,
     "status": "ok",
     "timestamp": 1739671993472,
     "user": {
      "displayName": "Tobias Schnarr",
      "userId": "16156372285777759423"
     },
     "user_tz": -60
    },
    "id": "wOmbn1Y1_J10",
    "outputId": "04436c47-2a8e-4267-bbc4-fe086b6f4336"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n",
      "<ipython-input-24-eeb13b4a304f>:55: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed. Time range: 2024-01-02 09:30:00-05:00 to 2025-01-31 16:00:00-05:00\n",
      "Normalized DataFrame head:\n",
      "                               open      high       low     close    volume  \\\n",
      "2024-01-02 09:30:00-05:00 -2.726382 -2.640049 -2.728858 -2.634558  1.638501   \n",
      "2024-01-02 09:31:00-05:00 -2.638013 -2.619083 -2.677415 -2.684876  0.296677   \n",
      "2024-01-02 09:32:00-05:00 -2.685561 -2.685046 -2.733330 -2.736867  0.194567   \n",
      "2024-01-02 09:33:00-05:00 -2.735886 -2.742623 -2.798755 -2.800601  0.341730   \n",
      "2024-01-02 09:34:00-05:00 -2.804335 -2.805793 -2.855792 -2.863220  0.313165   \n",
      "\n",
      "                           sin_hour  cos_hour  sin_minute  cos_minute  \\\n",
      "2024-01-02 09:30:00-05:00  1.661895  1.590095    0.069269   -1.412409   \n",
      "2024-01-02 09:31:00-05:00  1.661895  1.590095   -0.079101   -1.404671   \n",
      "2024-01-02 09:32:00-05:00  1.661895  1.590095   -0.225845   -1.381544   \n",
      "2024-01-02 09:33:00-05:00  1.661895  1.590095   -0.369356   -1.343280   \n",
      "2024-01-02 09:34:00-05:00  1.661895  1.590095   -0.508061   -1.290299   \n",
      "\n",
      "                           day_of_week  day_of_month     month  day_of_year  \\\n",
      "2024-01-02 09:30:00-05:00    -1.000642     -1.569318 -1.405962    -1.523964   \n",
      "2024-01-02 09:31:00-05:00    -1.000642     -1.569318 -1.405962    -1.523964   \n",
      "2024-01-02 09:32:00-05:00    -1.000642     -1.569318 -1.405962    -1.523964   \n",
      "2024-01-02 09:33:00-05:00    -1.000642     -1.569318 -1.405962    -1.523964   \n",
      "2024-01-02 09:34:00-05:00    -1.000642     -1.569318 -1.405962    -1.523964   \n",
      "\n",
      "                              year symbol  \n",
      "2024-01-02 09:30:00-05:00 -0.29143   MSFT  \n",
      "2024-01-02 09:31:00-05:00 -0.29143   MSFT  \n",
      "2024-01-02 09:32:00-05:00 -0.29143   MSFT  \n",
      "2024-01-02 09:33:00-05:00 -0.29143   MSFT  \n",
      "2024-01-02 09:34:00-05:00 -0.29143   MSFT  \n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n#########################################\\n# 3. SEQUENCE CREATION: DIRECT MULTI-STEP FORECASTING\\n#########################################\\n\\ndef create_sequences_multi(data: np.ndarray, window_size: int, forecast_horizon: int):\\n    \"\"\"\\n    Erstelle Sequenzen, bei denen das Input die vergangenen \\'window_size\\' Minuten sind\\n    und das Target ein Vektor der nächsten \\'forecast_horizon\\' \"close\"-Werte ist.\\n    data: numpy array der Form (num_timesteps, num_features)\\n    Returns:\\n      X: Form (num_samples, window_size, num_features)\\n      y: Form (num_samples, forecast_horizon)\\n    \"\"\"\\n    X, y = [], []\\n    # Hinweis: Der Feature-Index 3 entspricht \"close\"\\n    for i in range(len(data) - window_size - forecast_horizon + 1):\\n        X.append(data[i:i+window_size])\\n        y.append(data[i+window_size:i+window_size+forecast_horizon, 3])\\n    return np.array(X), np.array(y)\\n\\nWINDOW_SIZE = 60       # Vergangene 60 Minuten als Input\\nforecast_horizon = 60  # Vorhersage der nächsten 60 Minuten direkt\\n\\ndata_array = normalized_df[features_to_normalize].values\\nX_multi, y_multi = create_sequences_multi(data_array, WINDOW_SIZE, forecast_horizon)\\nprint(\"X_multi shape:\", X_multi.shape, \"y_multi shape:\", y_multi.shape)\\n\\n#########################################\\n# 4. PYTORCH DATASET & MULTI-STEP LSTM MODEL\\n#########################################\\n\\nclass StockDataset(Dataset):\\n    def __init__(self, X, y):\\n        self.X = torch.tensor(X, dtype=torch.float32)\\n        self.y = torch.tensor(y, dtype=torch.float32)\\n    def __len__(self):\\n        return len(self.y)\\n    def __getitem__(self, idx):\\n        return self.X[idx], self.y[idx]\\n\\nclass MultiStepLSTMModel(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2):\\n        super(MultiStepLSTMModel, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.forecast_horizon = forecast_horizon\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\\n        # Die finale Fully Connected Layer gibt einen Vektor der Länge forecast_horizon aus\\n        self.fc = nn.Linear(hidden_size, forecast_horizon)\\n    def forward(self, x):\\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n        out, _ = self.lstm(x, (h0, c0))\\n        # Verwende den Output des letzten Zeitschritts\\n        out = self.fc(out[:, -1, :])\\n        return out  # Form: (batch_size, forecast_horizon)\\n\\ninput_size = len(features_to_normalize)\\nhidden_size = 128\\nnum_layers = 1\\noutput_size = forecast_horizon  # wird nicht explizit verwendet; fc gibt forecast_horizon aus\\nmodel = MultiStepLSTMModel(input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2)\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\n\\n#########################################\\n# 5. TRAINING DES MODELLS\\n#########################################\\n\\nBATCH_SIZE = 64\\nEPOCHS = 100\\n\\ndataset = StockDataset(X_multi, y_multi)\\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\\ncriterion = nn.MSELoss()\\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\\n\\nfor epoch in range(EPOCHS):\\n    model.train()\\n    total_loss = 0.0\\n    for batch_x, batch_y in train_loader:\\n        batch_x = batch_x.to(device)  # (batch, WINDOW_SIZE, input_size)\\n        batch_y = batch_y.to(device)  # (batch, forecast_horizon)\\n        optimizer.zero_grad()\\n        output = model(batch_x)       # (batch, forecast_horizon)\\n        loss = criterion(output, batch_y)\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item() * batch_x.size(0)\\n    avg_loss = total_loss / len(dataset)\\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.6f}\")\\n\\n# Optional: Möchtest du das Modell direkt in deinem Google Drive speichern (Drive gemountet in Colab),\\n# passe den Pfad wie folgt an:\\ndrive_path = \"drive/MyDrive/pecunia/trained_models\"\\nos.makedirs(drive_path, exist_ok=True)\\nmodel_save_path = os.path.join(drive_path, f\"trained_model_{stock_ticker}.pth\")\\n\\ntorch.save(model.state_dict(), model_save_path)\\nprint(f\"Model saved to {model_save_path}\")\\n\\n#########################################\\n# 6. FORECASTING (DIRECT MULTI-STEP)\\n#########################################\\n\\n# Verwende die letzten WINDOW_SIZE Zeilen der normalisierten Daten als Input-Sequenz\\nlast_sequence = data_array[-WINDOW_SIZE:]  # Form: (WINDOW_SIZE, num_features)\\ninitial_sequence = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(device)\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    predicted_norm = model(initial_sequence).cpu().numpy().flatten()  # Form: (forecast_horizon,)\\n\\n# Inverse-Transformation der vorhergesagten normalisierten \"close\"-Werte.\\n# Es wird ein Platzhalter-Array für jeden vorhergesagten Schritt mit der korrekten Anzahl an Features erstellt.\\npred_placeholder = np.zeros((len(predicted_norm), len(features_to_normalize)))\\n# Setze die \"close\"-Spalte (Index 3) auf den vorhergesagten normalisierten Wert.\\npred_placeholder[:, 3] = predicted_norm\\n# Inverse transformieren mittels des Scalers. Nur die \"close\"-Spalte ist hierbei von Bedeutung.\\npredicted_prices = scaler.inverse_transform(pred_placeholder)[:, 3]\\nprint(\"Predicted Prices (Original Scale):\", predicted_prices)\\n\\n#########################################\\n# 7. PLOTTING DER PREDICTIONS\\n#########################################\\n\\n# Plot der letzten 200 Minuten historischer Close-Preise und Overlay der Vorhersage.\\nhistorical_data = processed_df[[\\'close\\']].iloc[-200:]\\n# Erstelle einen Zeitbereich für die Vorhersage (nächste 60 Minuten)\\nlast_time = historical_data.index[-1]\\nforecast_times = pd.date_range(last_time + pd.Timedelta(minutes=1), periods=forecast_horizon, freq=\\'T\\')\\n\\nplt.figure(figsize=(12, 6))\\nplt.plot(historical_data.index, historical_data[\\'close\\'], label=\"Historical Close Price\", lw=2)\\nplt.plot(forecast_times, predicted_prices, label=\"Forecasted Close Price\", lw=2, marker=\\'o\\')\\nplt.axvline(x=historical_data.index[-1], color=\"gray\", linestyle=\"--\", lw=1, label=\"Forecast Start\")\\nplt.title(f\"{stock_ticker} Close Price Forecast (Next 60 Minutes)\")\\nplt.xlabel(\"Time\")\\nplt.ylabel(\"Price (USD)\")\\nplt.legend()\\nplt.grid(True)\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the stock ticker\n",
    "stock_ticker = \"MSFT\"\n",
    "\n",
    "#########################################\n",
    "# 1. DATA LOADING, PROCESSING & FEATURE ENGINEERING\n",
    "#########################################\n",
    "\n",
    "def load_stock_data(file_pattern: str) -> pd.DataFrame:\n",
    "    all_dfs = []\n",
    "    files = glob.glob(file_pattern)\n",
    "    for file in tqdm(files, desc=\"Loading JSON files\"):\n",
    "        # Process only files that start with the stock ticker\n",
    "        if not os.path.basename(file).startswith(stock_ticker):\n",
    "            continue\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                metadata = data[\"Meta Data\"]\n",
    "                symbol = metadata[\"2. Symbol\"]\n",
    "                interval = metadata[\"4. Interval\"]\n",
    "                if interval != \"1min\":\n",
    "                    print(f\"Warning: {file} has interval {interval} (expected '1min'). Skipping.\")\n",
    "                    continue\n",
    "                ts_key = f\"Time Series ({interval})\"\n",
    "                ts_data = data.get(ts_key)\n",
    "                if not ts_data:\n",
    "                    print(f\"Warning: {ts_key} not found in {file}. Skipping.\")\n",
    "                    continue\n",
    "                df = pd.DataFrame(ts_data).T\n",
    "                df = df.apply(pd.to_numeric)\n",
    "                df[\"symbol\"] = symbol\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No valid data found in any files.\")\n",
    "    combined_df = pd.concat(all_dfs)\n",
    "    combined_df.index = pd.to_datetime(combined_df.index)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    # Remove any prefix (e.g. \"1. open\" -> \"open\") from column names\n",
    "    combined_df.columns = [col.split(\". \")[-1] for col in combined_df.columns]\n",
    "    return combined_df\n",
    "\n",
    "def process_stock_data(df: pd.DataFrame, fill_method: str = 'ffill',\n",
    "                       filter_market_hours: bool = True, timezone: str = 'US/Eastern') -> pd.DataFrame:\n",
    "    # Ensure index is timezone-localized\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(timezone)\n",
    "    processed_dfs = []\n",
    "    for symbol, group in df.groupby('symbol'):\n",
    "        # Remove duplicate timestamps (keep the last occurrence)\n",
    "        group = group[~group.index.duplicated(keep='last')]\n",
    "        # Reindex to a full minute range\n",
    "        full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='1T', tz=timezone)\n",
    "        group = group.reindex(full_range)\n",
    "        group['symbol'] = symbol\n",
    "        # Fill missing values as specified\n",
    "        if fill_method == 'ffill':\n",
    "            group = group.ffill()\n",
    "        elif fill_method == 'bfill':\n",
    "            group = group.bfill()\n",
    "        elif fill_method == 'interpolate':\n",
    "            group = group.interpolate()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid fill_method: {fill_method}.\")\n",
    "        # Clean the data by filtering out extended hours:\n",
    "        if filter_market_hours:\n",
    "            group = group.between_time('09:30', '16:00')\n",
    "        processed_dfs.append(group)\n",
    "    processed_df = pd.concat(processed_dfs)\n",
    "    return processed_df\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize('US/Eastern')\n",
    "    # Intraday cyclical features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['minute'] = df.index.minute\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['sin_minute'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "    df['cos_minute'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "    # Daily features\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['day_of_month'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    df['year'] = df.index.year\n",
    "    return df\n",
    "\n",
    "# Load minute-level JSON data\n",
    "raw_df = load_stock_data(f\"drive/MyDrive/software-engineering/alphavantage/data/{stock_ticker}_*.json\")\n",
    "\n",
    "# Process the data (this will filter out extended hours since filter_market_hours=True)\n",
    "processed_df = process_stock_data(raw_df, fill_method='ffill', filter_market_hours=True)\n",
    "processed_df = add_time_features(processed_df)\n",
    "\n",
    "# Optionally, save the cleaned/processed data to CSV for later use\n",
    "processed_df.to_csv(f\"drive/MyDrive/software-engineering/processed_data_{stock_ticker}.csv\", index_label='timestamp')\n",
    "print(\"Data processed. Time range:\", processed_df.index.min(), \"to\", processed_df.index.max())\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 2. NORMALIZATION\n",
    "#########################################\n",
    "\n",
    "# Definiere die Features in einer bestimmten Reihenfolge:\n",
    "# Preisfelder (open, high, low, close, volume), dann intraday cyclical, dann daily.\n",
    "features_to_normalize = ['open', 'high', 'low', 'close', 'volume',\n",
    "                           'sin_hour', 'cos_hour', 'sin_minute', 'cos_minute',\n",
    "                           'day_of_week', 'day_of_month', 'month', 'day_of_year', 'year']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_values = scaler.fit_transform(processed_df[features_to_normalize])\n",
    "normalized_df = pd.DataFrame(normalized_values, columns=features_to_normalize, index=processed_df.index)\n",
    "normalized_df['symbol'] = processed_df['symbol']\n",
    "print(\"Normalized DataFrame head:\")\n",
    "print(normalized_df.head())\n",
    "joblib.dump(scaler, f\"scaler_minute_{stock_ticker}.pkl\")\n",
    "\n",
    "#########################################\n",
    "# 3. SEQUENCE CREATION: DIRECT MULTI-STEP FORECASTING\n",
    "#########################################\n",
    "\n",
    "def create_sequences_multi(data: np.ndarray, window_size: int, forecast_horizon: int):\n",
    "    \"\"\"\n",
    "    Erstelle Sequenzen, bei denen das Input die vergangenen 'window_size' Minuten sind\n",
    "    und das Target ein Vektor der nächsten 'forecast_horizon' \"close\"-Werte ist.\n",
    "    data: numpy array der Form (num_timesteps, num_features)\n",
    "    Returns:\n",
    "      X: Form (num_samples, window_size, num_features)\n",
    "      y: Form (num_samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    # Hinweis: Der Feature-Index 3 entspricht \"close\"\n",
    "    for i in range(len(data) - window_size - forecast_horizon + 1):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size:i+window_size+forecast_horizon, 3])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "WINDOW_SIZE = 60       # Vergangene 60 Minuten als Input\n",
    "forecast_horizon = 60  # Vorhersage der nächsten 60 Minuten direkt\n",
    "\n",
    "data_array = normalized_df[features_to_normalize].values\n",
    "X_multi, y_multi = create_sequences_multi(data_array, WINDOW_SIZE, forecast_horizon)\n",
    "print(\"X_multi shape:\", X_multi.shape, \"y_multi shape:\", y_multi.shape)\n",
    "\n",
    "#########################################\n",
    "# 4. PYTORCH DATASET & MULTI-STEP LSTM MODEL\n",
    "#########################################\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class MultiStepLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2):\n",
    "        super(MultiStepLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        # Die finale Fully Connected Layer gibt einen Vektor der Länge forecast_horizon aus\n",
    "        self.fc = nn.Linear(hidden_size, forecast_horizon)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Verwende den Output des letzten Zeitschritts\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out  # Form: (batch_size, forecast_horizon)\n",
    "\n",
    "input_size = len(features_to_normalize)\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = forecast_horizon  # wird nicht explizit verwendet; fc gibt forecast_horizon aus\n",
    "model = MultiStepLSTMModel(input_size, hidden_size, num_layers, forecast_horizon, dropout=0.2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#########################################\n",
    "# 5. TRAINING DES MODELLS\n",
    "#########################################\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "dataset = StockDataset(X_multi, y_multi)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)  # (batch, WINDOW_SIZE, input_size)\n",
    "        batch_y = batch_y.to(device)  # (batch, forecast_horizon)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)       # (batch, forecast_horizon)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# speichere das trainierte Modell\n",
    "drive_path = \"drive/MyDrive/pecunia/trained_models\"\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "model_save_path = os.path.join(drive_path, f\"trained_model_{stock_ticker}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "#########################################\n",
    "# 6. FORECASTING (DIRECT MULTI-STEP)\n",
    "#########################################\n",
    "\n",
    "# Verwende die letzten WINDOW_SIZE Zeilen der normalisierten Daten als Input-Sequenz\n",
    "last_sequence = data_array[-WINDOW_SIZE:]\n",
    "initial_sequence = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_norm = model(initial_sequence).cpu().numpy().flatten()\n",
    "\n",
    "# Inverse-Transformation der vorhergesagten normalisierten \"close\"-Werte.\n",
    "\n",
    "pred_placeholder = np.zeros((len(predicted_norm), len(features_to_normalize)))\n",
    "\n",
    "pred_placeholder[:, 3] = predicted_norm\n",
    "# Inverse transformieren mittels des Scalers.\n",
    "predicted_prices = scaler.inverse_transform(pred_placeholder)[:, 3]\n",
    "print(\"Predicted Prices (Original Scale):\", predicted_prices)\n",
    "\n",
    "#########################################\n",
    "# 7. PLOTTING DER PREDICTIONS\n",
    "#########################################\n",
    "\n",
    "# Plot der letzten 200 Minuten historischer Close-Preise und Overlay der Vorhersage.\n",
    "historical_data = processed_df[['close']].iloc[-200:]\n",
    "# Erstelle einen Zeitbereich für die Vorhersage (nächste 60 Minuten)\n",
    "last_time = historical_data.index[-1]\n",
    "forecast_times = pd.date_range(last_time + pd.Timedelta(minutes=1), periods=forecast_horizon, freq='T')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(historical_data.index, historical_data['close'], label=\"Historical Close Price\", lw=2)\n",
    "plt.plot(forecast_times, predicted_prices, label=\"Forecasted Close Price\", lw=2, marker='o')\n",
    "plt.axvline(x=historical_data.index[-1], color=\"gray\", linestyle=\"--\", lw=1, label=\"Forecast Start\")\n",
    "plt.title(f\"{stock_ticker} Close Price Forecast (Next 60 Minutes)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN3y7s8r9amSz28zPsdvDu9",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
