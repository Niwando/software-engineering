{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1738959918249,
     "user": {
      "displayName": "Tobias Schnarr",
      "userId": "16156372285777759423"
     },
     "user_tz": -60
    },
    "id": "g6Ebel0G2_nf"
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, Dropout, Embedding, Concatenate, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yaQ02gzdFUTU",
    "outputId": "e24b1df1-94b0-4df1-bb8b-c64336ae077a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files:   0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files: 100%|██████████| 120/120 [00:24<00:00,  4.98it/s]\n",
      "/var/folders/cb/cr7qztjn4fqggln2x4_11n3c0000gn/T/ipykernel_4768/3552351602.py:97: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_range = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Verification ===\n",
      "Time range: 2024-01-02 09:30:00-05:00 to 2024-12-31 16:00:00-05:00\n",
      "Missing values:\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "symbol    0\n",
      "dtype: int64\n",
      "Unique symbols: ['AAPL' 'AMZN' 'AVGO' 'GOOGL' 'META' 'MSFT' 'NFLX' 'NVDA' 'PYPL' 'TSLA']\n",
      "Total rows: 1427150\n",
      "Data saved to processed_stock_data.csv\n",
      "Normalized DataFrame:\n",
      "                               open      high       low     close    volume  \\\n",
      "2024-01-02 09:30:00-05:00 -0.803345 -0.770856 -0.831948 -0.777085  2.639778   \n",
      "2024-01-02 09:31:00-05:00 -0.776965 -0.768140 -0.782270 -0.779607  0.589408   \n",
      "2024-01-02 09:32:00-05:00 -0.779292 -0.755731 -0.778390 -0.758914  0.486502   \n",
      "2024-01-02 09:33:00-05:00 -0.758537 -0.760770 -0.775285 -0.764282  0.317137   \n",
      "2024-01-02 09:34:00-05:00 -0.764552 -0.766979 -0.780718 -0.780964  0.258956   \n",
      "\n",
      "                          symbol  \n",
      "2024-01-02 09:30:00-05:00   AAPL  \n",
      "2024-01-02 09:31:00-05:00   AAPL  \n",
      "2024-01-02 09:32:00-05:00   AAPL  \n",
      "2024-01-02 09:33:00-05:00   AAPL  \n",
      "2024-01-02 09:34:00-05:00   AAPL  \n",
      "Input shape: (1426550, 60)\n",
      "Target shape: (1426550,)\n",
      "Symbol shape: (1426550,)\n",
      "Unseen data shape: (10, 60)\n",
      "Reshaped training data: (1141240, 60, 1), symbols: (1141240, 1)\n",
      "Reshaped validation data: (285310, 60, 1), symbols: (285310, 1)\n",
      "Reshaped unseen data: (10, 60, 1), symbols: (10, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m35662/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 401\u001b[0m\n\u001b[1;32m    395\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# 5. Training the Model\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_x_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol_x_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_y_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_x_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol_x_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_y_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# (Optional) Prediction on unseen sequences for each symbol\u001b[39;00m\n\u001b[1;32m    410\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([data_x_unseen, symbol_x_unseen])\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    386\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    387\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    407\u001b[0m }\n\u001b[1;32m    408\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:484\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    483\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 484\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/software-engineering/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. Data Loading, Processing, and Saving\n",
    "############################################\n",
    "\n",
    "def load_stock_data(file_pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads JSON files with 1-minute stock data, extracts metadata, and ensures the correct time series key.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: Glob pattern for JSON files (e.g., \"data/*.json\").\n",
    "\n",
    "    Returns:\n",
    "        Combined DataFrame with columns: [open, high, low, close, volume, symbol].\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    for file in tqdm(files, desc=\"Loading JSON files\"):\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                # Extract metadata\n",
    "                metadata = data[\"Meta Data\"]\n",
    "                symbol = metadata[\"2. Symbol\"]\n",
    "\n",
    "                # Verify the interval is \"1min\"\n",
    "                interval = metadata[\"4. Interval\"]\n",
    "                if interval != \"1min\":\n",
    "                    print(f\"Warning: {file} has interval {interval} (expected '1min'). Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Construct the time series key\n",
    "                ts_key = f\"Time Series ({interval})\"\n",
    "                ts_data = data.get(ts_key)\n",
    "\n",
    "                if not ts_data:\n",
    "                    print(f\"Warning: {ts_key} not found in {file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert time series to DataFrame\n",
    "                df = pd.DataFrame(ts_data).T\n",
    "                df = df.apply(pd.to_numeric)\n",
    "                df[\"symbol\"] = symbol\n",
    "                all_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No valid data found in any files.\")\n",
    "\n",
    "    # Combine DataFrames\n",
    "    combined_df = pd.concat(all_dfs)\n",
    "    combined_df.index = pd.to_datetime(combined_df.index)\n",
    "    combined_df = combined_df.sort_index()\n",
    "\n",
    "    # Rename columns (e.g., \"1. open\" → \"open\")\n",
    "    combined_df.columns = [col.split(\". \")[-1] for col in combined_df.columns]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def process_stock_data(\n",
    "    df: pd.DataFrame,\n",
    "    fill_method: str = 'ffill',\n",
    "    filter_market_hours: bool = True,\n",
    "    timezone: str = 'US/Eastern'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes stock data:\n",
    "    - Removes duplicate timestamps.\n",
    "    - Fills missing intervals.\n",
    "    - Optionally filters to market hours.\n",
    "\n",
    "    Args:\n",
    "        df: Raw DataFrame from `load_stock_data`.\n",
    "        fill_method: Method to fill missing data ('ffill', 'bfill', 'interpolate').\n",
    "        filter_market_hours: If True, retain only 9:30 AM - 4:00 PM ET timestamps.\n",
    "        timezone: Time zone to localize timestamps.\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame with continuous timestamps.\n",
    "    \"\"\"\n",
    "    # Localize timezone if not already set\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(timezone)\n",
    "\n",
    "    processed_dfs = []\n",
    "\n",
    "    # Process each symbol separately\n",
    "    for symbol, group in df.groupby('symbol'):\n",
    "        # Remove duplicates (keep last occurrence)\n",
    "        group = group[~group.index.duplicated(keep='last')]\n",
    "\n",
    "        # Create full time range for the symbol's period\n",
    "        full_range = pd.date_range(\n",
    "            start=group.index.min(),\n",
    "            end=group.index.max(),\n",
    "            freq='1T',\n",
    "            tz=timezone\n",
    "        )\n",
    "\n",
    "        # Reindex to fill missing times\n",
    "        group = group.reindex(full_range)\n",
    "\n",
    "        # Fill missing values\n",
    "        group['symbol'] = symbol  # Ensure symbol is preserved\n",
    "        if fill_method == 'ffill':\n",
    "            group = group.ffill()\n",
    "        elif fill_method == 'bfill':\n",
    "            group = group.bfill()\n",
    "        elif fill_method == 'interpolate':\n",
    "            group = group.interpolate()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid fill_method: {fill_method}. Use 'ffill', 'bfill', or 'interpolate'.\")\n",
    "\n",
    "        # Filter market hours (9:30 AM to 4:00 PM ET)\n",
    "        if filter_market_hours:\n",
    "            group = group.between_time('09:30', '16:00')\n",
    "\n",
    "        processed_dfs.append(group)\n",
    "\n",
    "    # Combine all symbols\n",
    "    processed_df = pd.concat(processed_dfs)\n",
    "    return processed_df\n",
    "\n",
    "def save_data(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Saves DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(output_path, index_label='timestamp')\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "def verify_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Prints basic checks for data integrity.\"\"\"\n",
    "    print(\"=== Data Verification ===\")\n",
    "    print(f\"Time range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Unique symbols: {df['symbol'].unique()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Step 1: Load raw data\n",
    "raw_df = load_stock_data(\"../alphavantage/data/*.json\")\n",
    "# raw_df = load_stock_data(\"data/*.json\")\n",
    "\n",
    "# Step 2: Process data\n",
    "processed_df = process_stock_data(\n",
    "    raw_df,\n",
    "    fill_method='ffill',  # Forward-fill missing values\n",
    "    filter_market_hours=True\n",
    ")\n",
    "\n",
    "# Step 3: Verify\n",
    "verify_data(processed_df)\n",
    "\n",
    "# Step 4: Save (optional)\n",
    "save_data(processed_df, \"processed_stock_data.csv\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# 2. Normalization\n",
    "############################################\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"\n",
    "        Normalizes the input data.\n",
    "        Args:\n",
    "            data: 2D numpy array or DataFrame (samples, features).\n",
    "        Returns:\n",
    "            Normalized data (numpy array).\n",
    "        \"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        self.mu = np.mean(data, axis=0, keepdims=True)\n",
    "        self.sd = np.std(data, axis=0, keepdims=True)\n",
    "        normalized_data = (data - self.mu) / self.sd\n",
    "        return normalized_data\n",
    "\n",
    "    def inverse_transform(self, normalized_data):\n",
    "        \"\"\"Converts normalized data back to original scale.\"\"\"\n",
    "        return (normalized_data * self.sd) + self.mu\n",
    "\n",
    "# Define features to normalize\n",
    "features_to_normalize = ['open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "# Initialize a dictionary to store scalers for each symbol\n",
    "scalers = {}\n",
    "normalized_dfs = []\n",
    "\n",
    "# Normalize each symbol's data\n",
    "for symbol, group in processed_df.groupby('symbol'):\n",
    "    scaler = Normalizer()\n",
    "    normalized_data = scaler.fit_transform(group[features_to_normalize])\n",
    "    scalers[symbol] = scaler\n",
    "    normalized_df = pd.DataFrame(\n",
    "        normalized_data,\n",
    "        columns=features_to_normalize,\n",
    "        index=group.index\n",
    "    )\n",
    "    # Preserve the symbol information\n",
    "    normalized_df['symbol'] = symbol\n",
    "    normalized_dfs.append(normalized_df)\n",
    "\n",
    "# Combine normalized DataFrames\n",
    "normalized_df = pd.concat(normalized_dfs)\n",
    "\n",
    "print(\"Normalized DataFrame:\")\n",
    "print(normalized_df.head())\n",
    "\n",
    "\n",
    "############################################\n",
    "# 3. Preparing Sequences (Including Symbol Labels)\n",
    "############################################\n",
    "\n",
    "def prepare_data_x(x, window_size):\n",
    "    \"\"\"\n",
    "    Create input sequences (X) using sliding windows.\n",
    "    Args:\n",
    "        x: 1D numpy array (normalized close prices).\n",
    "        window_size: Size of the sliding window.\n",
    "    Returns:\n",
    "        data_x: Input sequences for training/validation.\n",
    "        data_x_unseen: Final window for prediction.\n",
    "    \"\"\"\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    # Use stride_tricks for efficient windowing\n",
    "    output = np.lib.stride_tricks.as_strided(\n",
    "        x,\n",
    "        shape=(n_row, window_size),\n",
    "        strides=(x.strides[0], x.strides[0])\n",
    "    )\n",
    "    return output[:-1], output[-1:]  # All but last for training, last for prediction\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    \"\"\"\n",
    "    Create target values (y) for each window.\n",
    "    Args:\n",
    "        x: 1D numpy array (normalized close prices).\n",
    "        window_size: Size of the sliding window.\n",
    "    Returns:\n",
    "        output: Target values (the next time step after each window).\n",
    "    \"\"\"\n",
    "    return x[window_size:]  # Next time step for each window\n",
    "\n",
    "# Define parameters\n",
    "WINDOW_SIZE = 60    # Number of time steps per sequence\n",
    "TRAIN_SPLIT_SIZE = 0.8\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create lists to store sequences for all symbols.\n",
    "# We now also prepare arrays to hold the corresponding symbol (as an integer) for each sequence.\n",
    "all_data_x, all_data_y, all_symbol_x = [], [], []\n",
    "all_data_x_unseen, all_symbol_x_unseen = [], []\n",
    "\n",
    "# Create a mapping from symbol string to a unique integer (for the embedding).\n",
    "symbols = sorted(normalized_df['symbol'].unique())\n",
    "symbol_to_int = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "# For each symbol, prepare sliding window sequences.\n",
    "for symbol, group in normalized_df.groupby('symbol'):\n",
    "    # Extract the normalized close price values\n",
    "    normalized_data_close_price = group['close'].values\n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, window_size=WINDOW_SIZE)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, window_size=WINDOW_SIZE)\n",
    "\n",
    "    # Create an array for the symbol identifier (using our mapping)\n",
    "    symbol_code = symbol_to_int[symbol]\n",
    "    symbol_array = np.full((data_x.shape[0],), symbol_code, dtype=np.int32)\n",
    "    symbol_array_unseen = np.array([symbol_code], dtype=np.int32)  # one per symbol for unseen data\n",
    "\n",
    "    all_data_x.append(data_x)\n",
    "    all_data_y.append(data_y)\n",
    "    all_symbol_x.append(symbol_array)\n",
    "    all_data_x_unseen.append(data_x_unseen)\n",
    "    all_symbol_x_unseen.append(symbol_array_unseen)\n",
    "\n",
    "# Combine sequences from all symbols\n",
    "data_x = np.concatenate(all_data_x, axis=0)           # shape: (samples, WINDOW_SIZE)\n",
    "data_y = np.concatenate(all_data_y, axis=0)           # shape: (samples,)\n",
    "symbol_x = np.concatenate(all_symbol_x, axis=0)       # shape: (samples,)\n",
    "data_x_unseen = np.concatenate(all_data_x_unseen, axis=0)  # shape: (num_symbols, WINDOW_SIZE)\n",
    "symbol_x_unseen = np.concatenate(all_symbol_x_unseen, axis=0)  # shape: (num_symbols,)\n",
    "\n",
    "print(f\"Input shape: {data_x.shape}\")\n",
    "print(f\"Target shape: {data_y.shape}\")\n",
    "print(f\"Symbol shape: {symbol_x.shape}\")\n",
    "print(f\"Unseen data shape: {data_x_unseen.shape}\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_index = int(data_y.shape[0] * TRAIN_SPLIT_SIZE)\n",
    "data_x_train = data_x[:split_index]\n",
    "data_x_val = data_x[split_index:]\n",
    "data_y_train = data_y[:split_index]\n",
    "data_y_val = data_y[split_index:]\n",
    "symbol_x_train = symbol_x[:split_index]\n",
    "symbol_x_val = symbol_x[split_index:]\n",
    "\n",
    "# Reshape the sequence data for LSTM (adding feature dimension)\n",
    "data_x_train = data_x_train.reshape((data_x_train.shape[0], data_x_train.shape[1], 1))\n",
    "data_x_val = data_x_val.reshape((data_x_val.shape[0], data_x_val.shape[1], 1))\n",
    "data_x_unseen = data_x_unseen.reshape((data_x_unseen.shape[0], data_x_unseen.shape[1], 1))\n",
    "\n",
    "# The symbol inputs must be 2D (batch_size, 1) for the Embedding layer.\n",
    "symbol_x_train = symbol_x_train.reshape(-1, 1)\n",
    "symbol_x_val = symbol_x_val.reshape(-1, 1)\n",
    "symbol_x_unseen = symbol_x_unseen.reshape(-1, 1)\n",
    "\n",
    "print(f\"Reshaped training data: {data_x_train.shape}, symbols: {symbol_x_train.shape}\")\n",
    "print(f\"Reshaped validation data: {data_x_val.shape}, symbols: {symbol_x_val.shape}\")\n",
    "print(f\"Reshaped unseen data: {data_x_unseen.shape}, symbols: {symbol_x_unseen.shape}\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# 4. Multi-Input LSTM Model with Symbol Embedding\n",
    "############################################\n",
    "\n",
    "class MultiInputLSTMModel(Model):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2,\n",
    "                 output_size=1, dropout=0.2, num_symbols=10, embedding_dim=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of features in the sequence (here 1 for close price).\n",
    "            hidden_layer_size: Number of units in Dense and LSTM layers.\n",
    "            num_layers: Number of stacked LSTM layers.\n",
    "            output_size: Dimension of model output (1 for predicting a single value).\n",
    "            dropout: Dropout rate.\n",
    "            num_symbols: Total number of distinct symbols (for the Embedding layer).\n",
    "            embedding_dim: Dimension of the symbol embedding.\n",
    "        \"\"\"\n",
    "        super(MultiInputLSTMModel, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Layers to process the sequence input\n",
    "        self.linear_1 = Dense(hidden_layer_size, activation='relu')\n",
    "        self.lstm_layers = []\n",
    "        for i in range(num_layers):\n",
    "            # Return sequences for all but the last LSTM layer\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            self.lstm_layers.append(LSTM(hidden_layer_size, return_sequences=return_seq))\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        # Embedding layer for the stock symbol (categorical feature)\n",
    "        self.symbol_embedding = Embedding(input_dim=num_symbols, output_dim=embedding_dim, input_length=1)\n",
    "\n",
    "        # After concatenating LSTM output and symbol embedding, add a dense layer before final output.\n",
    "        self.combined_dense = Dense(16, activation='relu')\n",
    "        self.out_layer = Dense(output_size, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack the two inputs: sequence and symbol\n",
    "        seq_input, symbol_input = inputs\n",
    "\n",
    "        # Process the sequence input\n",
    "        x = self.linear_1(seq_input)\n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Process the symbol input via the embedding\n",
    "        symbol_emb = self.symbol_embedding(symbol_input)  # shape: (batch, 1, embedding_dim)\n",
    "        symbol_emb = tf.squeeze(symbol_emb, axis=1)         # shape: (batch, embedding_dim)\n",
    "\n",
    "        # Concatenate the LSTM features with the symbol embedding\n",
    "        combined = Concatenate()([x, symbol_emb])\n",
    "        combined = self.combined_dense(combined)\n",
    "        output = self.out_layer(combined)\n",
    "        return output\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 1                 # Only the close price is used in the sequence\n",
    "hidden_layer_size = 32\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "num_symbols = len(symbols)     # Based on the mapping we created\n",
    "embedding_dim = 4\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiInputLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout=dropout,\n",
    "    num_symbols=num_symbols,\n",
    "    embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "############################################\n",
    "# 5. Training the Model\n",
    "############################################\n",
    "\n",
    "history = model.fit(\n",
    "    [data_x_train, symbol_x_train],\n",
    "    data_y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=([data_x_val, symbol_x_val], data_y_val)\n",
    ")\n",
    "\n",
    "# (Optional) Prediction on unseen sequences for each symbol\n",
    "predictions = model.predict([data_x_unseen, symbol_x_unseen])\n",
    "print(\"Predictions on unseen data (one per symbol):\")\n",
    "for sym, pred in zip(symbols, predictions):\n",
    "    print(f\"{sym}: {pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oc8jDrWNFte"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcTRRXIWNMC8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# If your model uses custom layers or a custom model class (like our MultiInputLSTMModel),\n",
    "# you may need to supply a custom_objects dictionary:\n",
    "custom_objects = {\"MultiInputLSTMModel\": MultiInputLSTMModel}\n",
    "loaded_model = load_model(\"trained_model.h5\", custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxg8mqwLMuWq"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "model.save_weights(\"trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8gOY88oMxjo"
   },
   "outputs": [],
   "source": [
    "# Reconstruct your model (must be identical to the one you trained)\n",
    "model = MultiInputLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout=dropout,\n",
    "    num_symbols=num_symbols,\n",
    "    embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Load the weights\n",
    "model.load_weights(\"trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAYcGKCGMTwW"
   },
   "outputs": [],
   "source": [
    "# Define the number of points to plot (last (plot_range-1) actual values + predicted next day)\n",
    "plot_range = 10\n",
    "\n",
    "# Get the unique symbols from your normalized DataFrame\n",
    "symbols = sorted(normalized_df[\"symbol\"].unique())\n",
    "\n",
    "# Loop over each symbol to predict and plot\n",
    "for symbol in symbols:\n",
    "    # Filter for the specific symbol and sort by timestamp\n",
    "    df_symbol = normalized_df[normalized_df[\"symbol\"] == symbol].sort_index()\n",
    "\n",
    "    # Ensure there is enough data to form a prediction sequence\n",
    "    if len(df_symbol) < WINDOW_SIZE:\n",
    "        print(f\"Not enough data for symbol {symbol}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare the actual historical values for plotting\n",
    "    # ------------------------------------------------------\n",
    "    # We take the last (plot_range-1) normalized \"close\" values\n",
    "    actual_vals_norm = df_symbol[\"close\"].values[-(plot_range-1):]\n",
    "    # Inverse-transform these values back to the original scale\n",
    "    actual_vals = scaler.inverse_transform(actual_vals_norm.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare the input sequence for prediction\n",
    "    # ------------------------------------------------------\n",
    "    # Use the last WINDOW_SIZE normalized \"close\" values as the input sequence\n",
    "    sequence = df_symbol[\"close\"].values[-WINDOW_SIZE:]\n",
    "    sequence = sequence.reshape(1, WINDOW_SIZE, 1)  # shape: (1, WINDOW_SIZE, 1)\n",
    "\n",
    "    # Retrieve the integer code for the symbol using our mapping\n",
    "    symbol_int = symbol_to_int[symbol]\n",
    "    symbol_input = np.array([[symbol_int]])  # shape: (1, 1)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Make the prediction for the next trading day\n",
    "    # ------------------------------------------------------\n",
    "    prediction_norm = model.predict([sequence, symbol_input])\n",
    "    # Extract the predicted normalized close price (shape: (1, 1))\n",
    "    predicted_next_norm = prediction_norm[0, 0]\n",
    "    # Inverse-transform the predicted value\n",
    "    predicted_next = scaler.inverse_transform(np.array([[predicted_next_norm]]))[0, 0]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare data arrays for plotting\n",
    "    # ------------------------------------------------------\n",
    "    # For the x-axis, we use simple index labels for the last (plot_range-1) days and \"tomorrow\"\n",
    "    x_labels = [str(idx) for idx in range(len(df_symbol) - (plot_range-1), len(df_symbol))]\n",
    "    x_labels.append(\"tomorrow\")\n",
    "\n",
    "    # For the actual values, we add a placeholder (None) for the next day\n",
    "    actual_plot = np.concatenate([actual_vals, [None]])\n",
    "    # For the predicted values, we assume the past predictions equal the actual values,\n",
    "    # and then we set the predicted value for tomorrow\n",
    "    pred_plot = np.concatenate([actual_vals, [predicted_next]])\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Create the plot for this symbol\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_labels, actual_plot, label=\"Actual prices\", marker=\".\", markersize=10, color=\"blue\")\n",
    "    plt.plot(x_labels, pred_plot, label=\"Predicted prices\", marker=\".\", markersize=10, color=\"orange\")\n",
    "    plt.title(f\"Predicted Close Price of Next Trading Day for {symbol}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Close Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the predicted price for this symbol\n",
    "    print(f\"Predicted close price for {symbol}: {predicted_next:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwiowWneTktTSSApY/tww6",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
