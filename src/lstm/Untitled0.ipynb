{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1738959918249,
     "user": {
      "displayName": "Tobias Schnarr",
      "userId": "16156372285777759423"
     },
     "user_tz": -60
    },
    "id": "g6Ebel0G2_nf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Input, Dense, LSTM, Dropout, Embedding, Concatenate, Activation)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, Dropout, Embedding, Concatenate, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yaQ02gzdFUTU",
    "outputId": "e24b1df1-94b0-4df1-bb8b-c64336ae077a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files: 100%|██████████| 120/120 [01:00<00:00,  1.99it/s]\n",
      "<ipython-input-31-d2c82a749934>:97: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_range = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Verification ===\n",
      "Time range: 2024-01-02 09:30:00-05:00 to 2024-12-31 16:00:00-05:00\n",
      "Missing values:\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "symbol    0\n",
      "dtype: int64\n",
      "Unique symbols: ['AAPL' 'AMZN' 'AVGO' 'GOOGL' 'META' 'MSFT' 'NFLX' 'NVDA' 'PYPL' 'TSLA']\n",
      "Total rows: 1427150\n",
      "Data saved to processed_stock_data.csv\n",
      "Normalized DataFrame:\n",
      "                               open      high       low     close    volume  \\\n",
      "2024-01-02 09:30:00-05:00 -0.803345 -0.770856 -0.831948 -0.777085  2.639778   \n",
      "2024-01-02 09:31:00-05:00 -0.776965 -0.768140 -0.782270 -0.779607  0.589408   \n",
      "2024-01-02 09:32:00-05:00 -0.779292 -0.755731 -0.778390 -0.758914  0.486502   \n",
      "2024-01-02 09:33:00-05:00 -0.758537 -0.760770 -0.775285 -0.764282  0.317137   \n",
      "2024-01-02 09:34:00-05:00 -0.764552 -0.766979 -0.780718 -0.780964  0.258956   \n",
      "\n",
      "                          symbol  \n",
      "2024-01-02 09:30:00-05:00   AAPL  \n",
      "2024-01-02 09:31:00-05:00   AAPL  \n",
      "2024-01-02 09:32:00-05:00   AAPL  \n",
      "2024-01-02 09:33:00-05:00   AAPL  \n",
      "2024-01-02 09:34:00-05:00   AAPL  \n",
      "Input shape: (1426550, 60)\n",
      "Target shape: (1426550,)\n",
      "Symbol shape: (1426550,)\n",
      "Unseen data shape: (10, 60)\n",
      "Reshaped training data: (1141240, 60, 1), symbols: (1141240, 1)\n",
      "Reshaped validation data: (285310, 60, 1), symbols: (285310, 1)\n",
      "Reshaped unseen data: (10, 60, 1), symbols: (10, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"multi_input_lstm_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"multi_input_lstm_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 11ms/step - loss: 0.0030 - val_loss: 1.1043e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 11ms/step - loss: 1.3177e-04 - val_loss: 8.2429e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 11ms/step - loss: 1.0357e-04 - val_loss: 7.5576e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 11ms/step - loss: 1.0883e-04 - val_loss: 1.0184e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 11ms/step - loss: 1.0218e-04 - val_loss: 1.7020e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 11ms/step - loss: 1.0366e-04 - val_loss: 7.2150e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 11ms/step - loss: 1.0049e-04 - val_loss: 6.9140e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 11ms/step - loss: 9.1356e-05 - val_loss: 8.9607e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 11ms/step - loss: 9.1753e-05 - val_loss: 6.4796e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 11ms/step - loss: 8.7261e-05 - val_loss: 9.3256e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 11ms/step - loss: 8.1995e-05 - val_loss: 9.0600e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 11ms/step - loss: 1.0191e-04 - val_loss: 6.4152e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 11ms/step - loss: 8.7693e-05 - val_loss: 6.4595e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 11ms/step - loss: 9.2295e-05 - val_loss: 6.9314e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 11ms/step - loss: 8.4923e-05 - val_loss: 6.1057e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 11ms/step - loss: 8.0659e-05 - val_loss: 6.8501e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 11ms/step - loss: 8.2506e-05 - val_loss: 9.2846e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 11ms/step - loss: 8.5546e-05 - val_loss: 6.2598e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 11ms/step - loss: 8.7344e-05 - val_loss: 5.9853e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 11ms/step - loss: 8.5814e-05 - val_loss: 6.2618e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 11ms/step - loss: 8.7162e-05 - val_loss: 6.2778e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 11ms/step - loss: 9.0782e-05 - val_loss: 1.5374e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 11ms/step - loss: 8.5387e-05 - val_loss: 7.5733e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 11ms/step - loss: 8.4185e-05 - val_loss: 6.1447e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m35664/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 11ms/step - loss: 8.5948e-05 - val_loss: 6.7895e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m27030/35664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 10ms/step - loss: 8.7523e-05"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. Data Loading, Processing, and Saving\n",
    "############################################\n",
    "\n",
    "def load_stock_data(file_pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads JSON files with 1-minute stock data, extracts metadata, and ensures the correct time series key.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: Glob pattern for JSON files (e.g., \"data/*.json\").\n",
    "\n",
    "    Returns:\n",
    "        Combined DataFrame with columns: [open, high, low, close, volume, symbol].\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    for file in tqdm(files, desc=\"Loading JSON files\"):\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                # Extract metadata\n",
    "                metadata = data[\"Meta Data\"]\n",
    "                symbol = metadata[\"2. Symbol\"]\n",
    "\n",
    "                # Verify the interval is \"1min\"\n",
    "                interval = metadata[\"4. Interval\"]\n",
    "                if interval != \"1min\":\n",
    "                    print(f\"Warning: {file} has interval {interval} (expected '1min'). Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Construct the time series key\n",
    "                ts_key = f\"Time Series ({interval})\"\n",
    "                ts_data = data.get(ts_key)\n",
    "\n",
    "                if not ts_data:\n",
    "                    print(f\"Warning: {ts_key} not found in {file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert time series to DataFrame\n",
    "                df = pd.DataFrame(ts_data).T\n",
    "                df = df.apply(pd.to_numeric)\n",
    "                df[\"symbol\"] = symbol\n",
    "                all_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No valid data found in any files.\")\n",
    "\n",
    "    # Combine DataFrames\n",
    "    combined_df = pd.concat(all_dfs)\n",
    "    combined_df.index = pd.to_datetime(combined_df.index)\n",
    "    combined_df = combined_df.sort_index()\n",
    "\n",
    "    # Rename columns (e.g., \"1. open\" → \"open\")\n",
    "    combined_df.columns = [col.split(\". \")[-1] for col in combined_df.columns]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def process_stock_data(\n",
    "    df: pd.DataFrame,\n",
    "    fill_method: str = 'ffill',\n",
    "    filter_market_hours: bool = True,\n",
    "    timezone: str = 'US/Eastern'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes stock data:\n",
    "    - Removes duplicate timestamps.\n",
    "    - Fills missing intervals.\n",
    "    - Optionally filters to market hours.\n",
    "\n",
    "    Args:\n",
    "        df: Raw DataFrame from `load_stock_data`.\n",
    "        fill_method: Method to fill missing data ('ffill', 'bfill', 'interpolate').\n",
    "        filter_market_hours: If True, retain only 9:30 AM - 4:00 PM ET timestamps.\n",
    "        timezone: Time zone to localize timestamps.\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame with continuous timestamps.\n",
    "    \"\"\"\n",
    "    # Localize timezone if not already set\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(timezone)\n",
    "\n",
    "    processed_dfs = []\n",
    "\n",
    "    # Process each symbol separately\n",
    "    for symbol, group in df.groupby('symbol'):\n",
    "        # Remove duplicates (keep last occurrence)\n",
    "        group = group[~group.index.duplicated(keep='last')]\n",
    "\n",
    "        # Create full time range for the symbol's period\n",
    "        full_range = pd.date_range(\n",
    "            start=group.index.min(),\n",
    "            end=group.index.max(),\n",
    "            freq='1T',\n",
    "            tz=timezone\n",
    "        )\n",
    "\n",
    "        # Reindex to fill missing times\n",
    "        group = group.reindex(full_range)\n",
    "\n",
    "        # Fill missing values\n",
    "        group['symbol'] = symbol  # Ensure symbol is preserved\n",
    "        if fill_method == 'ffill':\n",
    "            group = group.ffill()\n",
    "        elif fill_method == 'bfill':\n",
    "            group = group.bfill()\n",
    "        elif fill_method == 'interpolate':\n",
    "            group = group.interpolate()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid fill_method: {fill_method}. Use 'ffill', 'bfill', or 'interpolate'.\")\n",
    "\n",
    "        # Filter market hours (9:30 AM to 4:00 PM ET)\n",
    "        if filter_market_hours:\n",
    "            group = group.between_time('09:30', '16:00')\n",
    "\n",
    "        processed_dfs.append(group)\n",
    "\n",
    "    # Combine all symbols\n",
    "    processed_df = pd.concat(processed_dfs)\n",
    "    return processed_df\n",
    "\n",
    "def save_data(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Saves DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(output_path, index_label='timestamp')\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "def verify_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Prints basic checks for data integrity.\"\"\"\n",
    "    print(\"=== Data Verification ===\")\n",
    "    print(f\"Time range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Unique symbols: {df['symbol'].unique()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Step 1: Load raw data\n",
    "#raw_df = load_stock_data(\"../alphavantage/data/*.json\")\n",
    "raw_df = load_stock_data(\"data/*.json\")\n",
    "\n",
    "# Step 2: Process data\n",
    "processed_df = process_stock_data(\n",
    "    raw_df,\n",
    "    fill_method='ffill',  # Forward-fill missing values\n",
    "    filter_market_hours=True\n",
    ")\n",
    "\n",
    "# Step 3: Verify\n",
    "verify_data(processed_df)\n",
    "\n",
    "# Step 4: Save (optional)\n",
    "save_data(processed_df, \"processed_stock_data.csv\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# 2. Normalization\n",
    "############################################\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"\n",
    "        Normalizes the input data.\n",
    "        Args:\n",
    "            data: 2D numpy array or DataFrame (samples, features).\n",
    "        Returns:\n",
    "            Normalized data (numpy array).\n",
    "        \"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        self.mu = np.mean(data, axis=0, keepdims=True)\n",
    "        self.sd = np.std(data, axis=0, keepdims=True)\n",
    "        normalized_data = (data - self.mu) / self.sd\n",
    "        return normalized_data\n",
    "\n",
    "    def inverse_transform(self, normalized_data):\n",
    "        \"\"\"Converts normalized data back to original scale.\"\"\"\n",
    "        return (normalized_data * self.sd) + self.mu\n",
    "\n",
    "# Define features to normalize\n",
    "features_to_normalize = ['open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "# Initialize a dictionary to store scalers for each symbol\n",
    "scalers = {}\n",
    "normalized_dfs = []\n",
    "\n",
    "# Normalize each symbol's data\n",
    "for symbol, group in processed_df.groupby('symbol'):\n",
    "    scaler = Normalizer()\n",
    "    normalized_data = scaler.fit_transform(group[features_to_normalize])\n",
    "    scalers[symbol] = scaler\n",
    "    normalized_df = pd.DataFrame(\n",
    "        normalized_data,\n",
    "        columns=features_to_normalize,\n",
    "        index=group.index\n",
    "    )\n",
    "    # Preserve the symbol information\n",
    "    normalized_df['symbol'] = symbol\n",
    "    normalized_dfs.append(normalized_df)\n",
    "\n",
    "# Combine normalized DataFrames\n",
    "normalized_df = pd.concat(normalized_dfs)\n",
    "\n",
    "print(\"Normalized DataFrame:\")\n",
    "print(normalized_df.head())\n",
    "\n",
    "\n",
    "############################################\n",
    "# 3. Preparing Sequences (Including Symbol Labels)\n",
    "############################################\n",
    "\n",
    "def prepare_data_x(x, window_size):\n",
    "    \"\"\"\n",
    "    Create input sequences (X) using sliding windows.\n",
    "    Args:\n",
    "        x: 1D numpy array (normalized close prices).\n",
    "        window_size: Size of the sliding window.\n",
    "    Returns:\n",
    "        data_x: Input sequences for training/validation.\n",
    "        data_x_unseen: Final window for prediction.\n",
    "    \"\"\"\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    # Use stride_tricks for efficient windowing\n",
    "    output = np.lib.stride_tricks.as_strided(\n",
    "        x,\n",
    "        shape=(n_row, window_size),\n",
    "        strides=(x.strides[0], x.strides[0])\n",
    "    )\n",
    "    return output[:-1], output[-1:]  # All but last for training, last for prediction\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    \"\"\"\n",
    "    Create target values (y) for each window.\n",
    "    Args:\n",
    "        x: 1D numpy array (normalized close prices).\n",
    "        window_size: Size of the sliding window.\n",
    "    Returns:\n",
    "        output: Target values (the next time step after each window).\n",
    "    \"\"\"\n",
    "    return x[window_size:]  # Next time step for each window\n",
    "\n",
    "# Define parameters\n",
    "WINDOW_SIZE = 60    # Number of time steps per sequence\n",
    "TRAIN_SPLIT_SIZE = 0.8\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create lists to store sequences for all symbols.\n",
    "# We now also prepare arrays to hold the corresponding symbol (as an integer) for each sequence.\n",
    "all_data_x, all_data_y, all_symbol_x = [], [], []\n",
    "all_data_x_unseen, all_symbol_x_unseen = [], []\n",
    "\n",
    "# Create a mapping from symbol string to a unique integer (for the embedding).\n",
    "symbols = sorted(normalized_df['symbol'].unique())\n",
    "symbol_to_int = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "# For each symbol, prepare sliding window sequences.\n",
    "for symbol, group in normalized_df.groupby('symbol'):\n",
    "    # Extract the normalized close price values\n",
    "    normalized_data_close_price = group['close'].values\n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, window_size=WINDOW_SIZE)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, window_size=WINDOW_SIZE)\n",
    "\n",
    "    # Create an array for the symbol identifier (using our mapping)\n",
    "    symbol_code = symbol_to_int[symbol]\n",
    "    symbol_array = np.full((data_x.shape[0],), symbol_code, dtype=np.int32)\n",
    "    symbol_array_unseen = np.array([symbol_code], dtype=np.int32)  # one per symbol for unseen data\n",
    "\n",
    "    all_data_x.append(data_x)\n",
    "    all_data_y.append(data_y)\n",
    "    all_symbol_x.append(symbol_array)\n",
    "    all_data_x_unseen.append(data_x_unseen)\n",
    "    all_symbol_x_unseen.append(symbol_array_unseen)\n",
    "\n",
    "# Combine sequences from all symbols\n",
    "data_x = np.concatenate(all_data_x, axis=0)           # shape: (samples, WINDOW_SIZE)\n",
    "data_y = np.concatenate(all_data_y, axis=0)           # shape: (samples,)\n",
    "symbol_x = np.concatenate(all_symbol_x, axis=0)       # shape: (samples,)\n",
    "data_x_unseen = np.concatenate(all_data_x_unseen, axis=0)  # shape: (num_symbols, WINDOW_SIZE)\n",
    "symbol_x_unseen = np.concatenate(all_symbol_x_unseen, axis=0)  # shape: (num_symbols,)\n",
    "\n",
    "print(f\"Input shape: {data_x.shape}\")\n",
    "print(f\"Target shape: {data_y.shape}\")\n",
    "print(f\"Symbol shape: {symbol_x.shape}\")\n",
    "print(f\"Unseen data shape: {data_x_unseen.shape}\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_index = int(data_y.shape[0] * TRAIN_SPLIT_SIZE)\n",
    "data_x_train = data_x[:split_index]\n",
    "data_x_val = data_x[split_index:]\n",
    "data_y_train = data_y[:split_index]\n",
    "data_y_val = data_y[split_index:]\n",
    "symbol_x_train = symbol_x[:split_index]\n",
    "symbol_x_val = symbol_x[split_index:]\n",
    "\n",
    "# Reshape the sequence data for LSTM (adding feature dimension)\n",
    "data_x_train = data_x_train.reshape((data_x_train.shape[0], data_x_train.shape[1], 1))\n",
    "data_x_val = data_x_val.reshape((data_x_val.shape[0], data_x_val.shape[1], 1))\n",
    "data_x_unseen = data_x_unseen.reshape((data_x_unseen.shape[0], data_x_unseen.shape[1], 1))\n",
    "\n",
    "# The symbol inputs must be 2D (batch_size, 1) for the Embedding layer.\n",
    "symbol_x_train = symbol_x_train.reshape(-1, 1)\n",
    "symbol_x_val = symbol_x_val.reshape(-1, 1)\n",
    "symbol_x_unseen = symbol_x_unseen.reshape(-1, 1)\n",
    "\n",
    "print(f\"Reshaped training data: {data_x_train.shape}, symbols: {symbol_x_train.shape}\")\n",
    "print(f\"Reshaped validation data: {data_x_val.shape}, symbols: {symbol_x_val.shape}\")\n",
    "print(f\"Reshaped unseen data: {data_x_unseen.shape}, symbols: {symbol_x_unseen.shape}\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# 4. Multi-Input LSTM Model with Symbol Embedding\n",
    "############################################\n",
    "\n",
    "class MultiInputLSTMModel(Model):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2,\n",
    "                 output_size=1, dropout=0.2, num_symbols=10, embedding_dim=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of features in the sequence (here 1 for close price).\n",
    "            hidden_layer_size: Number of units in Dense and LSTM layers.\n",
    "            num_layers: Number of stacked LSTM layers.\n",
    "            output_size: Dimension of model output (1 for predicting a single value).\n",
    "            dropout: Dropout rate.\n",
    "            num_symbols: Total number of distinct symbols (for the Embedding layer).\n",
    "            embedding_dim: Dimension of the symbol embedding.\n",
    "        \"\"\"\n",
    "        super(MultiInputLSTMModel, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Layers to process the sequence input\n",
    "        self.linear_1 = Dense(hidden_layer_size, activation='relu')\n",
    "        self.lstm_layers = []\n",
    "        for i in range(num_layers):\n",
    "            # Return sequences for all but the last LSTM layer\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            self.lstm_layers.append(LSTM(hidden_layer_size, return_sequences=return_seq))\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        # Embedding layer for the stock symbol (categorical feature)\n",
    "        self.symbol_embedding = Embedding(input_dim=num_symbols, output_dim=embedding_dim, input_length=1)\n",
    "\n",
    "        # After concatenating LSTM output and symbol embedding, add a dense layer before final output.\n",
    "        self.combined_dense = Dense(16, activation='relu')\n",
    "        self.out_layer = Dense(output_size, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack the two inputs: sequence and symbol\n",
    "        seq_input, symbol_input = inputs\n",
    "\n",
    "        # Process the sequence input\n",
    "        x = self.linear_1(seq_input)\n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Process the symbol input via the embedding\n",
    "        symbol_emb = self.symbol_embedding(symbol_input)  # shape: (batch, 1, embedding_dim)\n",
    "        symbol_emb = tf.squeeze(symbol_emb, axis=1)         # shape: (batch, embedding_dim)\n",
    "\n",
    "        # Concatenate the LSTM features with the symbol embedding\n",
    "        combined = Concatenate()([x, symbol_emb])\n",
    "        combined = self.combined_dense(combined)\n",
    "        output = self.out_layer(combined)\n",
    "        return output\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 1                 # Only the close price is used in the sequence\n",
    "hidden_layer_size = 32\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "num_symbols = len(symbols)     # Based on the mapping we created\n",
    "embedding_dim = 4\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiInputLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout=dropout,\n",
    "    num_symbols=num_symbols,\n",
    "    embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "############################################\n",
    "# 5. Training the Model\n",
    "############################################\n",
    "\n",
    "history = model.fit(\n",
    "    [data_x_train, symbol_x_train],\n",
    "    data_y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=([data_x_val, symbol_x_val], data_y_val)\n",
    ")\n",
    "\n",
    "# (Optional) Prediction on unseen sequences for each symbol\n",
    "predictions = model.predict([data_x_unseen, symbol_x_unseen])\n",
    "print(\"Predictions on unseen data (one per symbol):\")\n",
    "for sym, pred in zip(symbols, predictions):\n",
    "    print(f\"{sym}: {pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oc8jDrWNFte"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcTRRXIWNMC8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# If your model uses custom layers or a custom model class (like our MultiInputLSTMModel),\n",
    "# you may need to supply a custom_objects dictionary:\n",
    "custom_objects = {\"MultiInputLSTMModel\": MultiInputLSTMModel}\n",
    "loaded_model = load_model(\"trained_model.h5\", custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxg8mqwLMuWq"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "model.save_weights(\"trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8gOY88oMxjo"
   },
   "outputs": [],
   "source": [
    "# Reconstruct your model (must be identical to the one you trained)\n",
    "model = MultiInputLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout=dropout,\n",
    "    num_symbols=num_symbols,\n",
    "    embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Load the weights\n",
    "model.load_weights(\"trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAYcGKCGMTwW"
   },
   "outputs": [],
   "source": [
    "# Define the number of points to plot (last (plot_range-1) actual values + predicted next day)\n",
    "plot_range = 10\n",
    "\n",
    "# Get the unique symbols from your normalized DataFrame\n",
    "symbols = sorted(normalized_df[\"symbol\"].unique())\n",
    "\n",
    "# Loop over each symbol to predict and plot\n",
    "for symbol in symbols:\n",
    "    # Filter for the specific symbol and sort by timestamp\n",
    "    df_symbol = normalized_df[normalized_df[\"symbol\"] == symbol].sort_index()\n",
    "\n",
    "    # Ensure there is enough data to form a prediction sequence\n",
    "    if len(df_symbol) < WINDOW_SIZE:\n",
    "        print(f\"Not enough data for symbol {symbol}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare the actual historical values for plotting\n",
    "    # ------------------------------------------------------\n",
    "    # We take the last (plot_range-1) normalized \"close\" values\n",
    "    actual_vals_norm = df_symbol[\"close\"].values[-(plot_range-1):]\n",
    "    # Inverse-transform these values back to the original scale\n",
    "    actual_vals = scaler.inverse_transform(actual_vals_norm.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare the input sequence for prediction\n",
    "    # ------------------------------------------------------\n",
    "    # Use the last WINDOW_SIZE normalized \"close\" values as the input sequence\n",
    "    sequence = df_symbol[\"close\"].values[-WINDOW_SIZE:]\n",
    "    sequence = sequence.reshape(1, WINDOW_SIZE, 1)  # shape: (1, WINDOW_SIZE, 1)\n",
    "\n",
    "    # Retrieve the integer code for the symbol using our mapping\n",
    "    symbol_int = symbol_to_int[symbol]\n",
    "    symbol_input = np.array([[symbol_int]])  # shape: (1, 1)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Make the prediction for the next trading day\n",
    "    # ------------------------------------------------------\n",
    "    prediction_norm = model.predict([sequence, symbol_input])\n",
    "    # Extract the predicted normalized close price (shape: (1, 1))\n",
    "    predicted_next_norm = prediction_norm[0, 0]\n",
    "    # Inverse-transform the predicted value\n",
    "    predicted_next = scaler.inverse_transform(np.array([[predicted_next_norm]]))[0, 0]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Prepare data arrays for plotting\n",
    "    # ------------------------------------------------------\n",
    "    # For the x-axis, we use simple index labels for the last (plot_range-1) days and \"tomorrow\"\n",
    "    x_labels = [str(idx) for idx in range(len(df_symbol) - (plot_range-1), len(df_symbol))]\n",
    "    x_labels.append(\"tomorrow\")\n",
    "\n",
    "    # For the actual values, we add a placeholder (None) for the next day\n",
    "    actual_plot = np.concatenate([actual_vals, [None]])\n",
    "    # For the predicted values, we assume the past predictions equal the actual values,\n",
    "    # and then we set the predicted value for tomorrow\n",
    "    pred_plot = np.concatenate([actual_vals, [predicted_next]])\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Create the plot for this symbol\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_labels, actual_plot, label=\"Actual prices\", marker=\".\", markersize=10, color=\"blue\")\n",
    "    plt.plot(x_labels, pred_plot, label=\"Predicted prices\", marker=\".\", markersize=10, color=\"orange\")\n",
    "    plt.title(f\"Predicted Close Price of Next Trading Day for {symbol}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Close Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the predicted price for this symbol\n",
    "    print(f\"Predicted close price for {symbol}: {predicted_next:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwiowWneTktTSSApY/tww6",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
